import { Model } from './types';

export const MODELS: Model[] = [
  {
    id: 'llama3-8b',
    name: 'Llama 3 8B Instruct',
    description: 'The latest 8B parameter language model from Meta, suitable for a wide range of tasks.',
    source: 'Ollama',
    size: 4.7,
    ram_required: 8,
    vram_required: 6,
    quantization: 'Q4_K_M',
    quality_score: 85,
    performance_score: 75,
    speed_score: 90,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'coding', 'chat'],
  },
  {
    id: 'mistral-7b',
    name: 'Mistral 7B',
    description: 'A popular 7B parameter model known for its high performance and efficiency.',
    source: 'HuggingFace',
    size: 4.1,
    ram_required: 8,
    vram_required: 6,
    quantization: 'Q4_0',
    quality_score: 82,
    performance_score: 80,
    speed_score: 92,
    cpu_performance_needed: 'low',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'efficient'],
  },
  {
    id: 'phi3-mini',
    name: 'Phi-3 Mini 3.8B',
    description: 'A powerful small language model from Microsoft, designed for on-device and offline use.',
    source: 'Ollama',
    size: 2.3,
    ram_required: 4,
    vram_required: 3,
    quantization: 'Q4_K_M',
    quality_score: 78,
    performance_score: 88,
    speed_score: 95,
    cpu_performance_needed: 'low',
    gpu_performance_needed: 'low',
    apple_silicon_support: true,
    tags: ['small', 'fast', 'on-device'],
  },
  {
    id: 'gemma-7b',
    name: 'Gemma 7B',
    description: 'A family of lightweight, state-of-the-art open models from Google.',
    source: 'HuggingFace',
    size: 4.8,
    ram_required: 8,
    vram_required: 6,
    quantization: 'GGUF',
    quality_score: 83,
    performance_score: 78,
    speed_score: 88,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'google', 'research'],
  },
  {
    id: 'codellama-34b',
    name: 'Code Llama 34B',
    description: 'A large language model specialized for code generation and understanding.',
    source: 'Ollama',
    size: 19.1,
    ram_required: 32,
    vram_required: 24,
    quantization: 'Q4_K_M',
    quality_score: 92,
    performance_score: 60,
    speed_score: 50,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['coding', 'large', 'specialized'],
  },
  {
    id: 'mixtral-8x7b',
    name: 'Mixtral 8x7B',
    description: 'A high-quality sparse Mixture-of-Experts model (SMoE) by Mistral AI.',
    source: 'HuggingFace',
    size: 26,
    ram_required: 48,
    vram_required: 32,
    quantization: 'Q4_0',
    quality_score: 95,
    performance_score: 65,
    speed_score: 60,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: true,
    tags: ['mixture-of-experts', 'high-quality'],
  },
  {
    id: 'qwen-72b',
    name: 'Qwen 1.5 72B',
    description: 'A massive 72B parameter model from Alibaba Cloud, excelling in multilingual tasks.',
    source: 'Ollama',
    size: 41,
    ram_required: 64,
    vram_required: 48,
    quantization: 'Q4_K_M',
    quality_score: 94,
    performance_score: 55,
    speed_score: 45,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['multilingual', 'large', 'chat'],
  },
  {
    id: 'llava-13b',
    name: 'LLaVA 1.5 13B',
    description: 'A large multi-modal model that connects a vision encoder and a large language model for general-purpose visual and language understanding.',
    source: 'HuggingFace',
    size: 7.4,
    ram_required: 16,
    vram_required: 10,
    quantization: 'GGUF',
    quality_score: 88,
    performance_score: 70,
    speed_score: 75,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['vision', 'multimodal', 'chat'],
  },
];
