import { Model } from './types';

export const MODELS: Model[] = [
  {
    id: 'llama3-8b',
    name: 'Llama 3 8B Instruct',
    description: 'The latest 8B parameter language model from Meta, suitable for a wide range of tasks.',
    source: 'Ollama',
    size: 4.7,
    ram_required: 8,
    vram_required: 6,
    quantization: 'Q4_K_M',
    quality_score: 85,
    performance_score: 75,
    speed_score: 90,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'coding', 'chat'],
  },
  {
    id: 'mistral-7b',
    name: 'Mistral 7B',
    description: 'A popular 7B parameter model known for its high performance and efficiency.',
    source: 'HuggingFace',
    size: 4.1,
    ram_required: 8,
    vram_required: 6,
    quantization: 'Q4_0',
    quality_score: 82,
    performance_score: 80,
    speed_score: 92,
    cpu_performance_needed: 'low',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'efficient'],
  },
  {
    id: 'phi3-mini',
    name: 'Phi-3 Mini 3.8B',
    description: 'A powerful small language model from Microsoft, designed for on-device and offline use.',
    source: 'Ollama',
    size: 2.3,
    ram_required: 4,
    vram_required: 3,
    quantization: 'Q4_K_M',
    quality_score: 78,
    performance_score: 88,
    speed_score: 95,
    cpu_performance_needed: 'low',
    gpu_performance_needed: 'low',
    apple_silicon_support: true,
    tags: ['small', 'fast', 'on-device'],
  },
  {
    id: 'gemma-7b',
    name: 'Gemma 7B',
    description: 'A family of lightweight, state-of-the-art open models from Google.',
    source: 'HuggingFace',
    size: 4.8,
    ram_required: 8,
    vram_required: 6,
    quantization: 'GGUF',
    quality_score: 83,
    performance_score: 78,
    speed_score: 88,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['general', 'google', 'research'],
  },
  {
    id: 'codellama-34b',
    name: 'Code Llama 34B',
    description: 'A large language model specialized for code generation and understanding.',
    source: 'Ollama',
    size: 19.1,
    ram_required: 32,
    vram_required: 24,
    quantization: 'Q4_K_M',
    quality_score: 92,
    performance_score: 60,
    speed_score: 50,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['coding', 'large', 'specialized'],
  },
  {
    id: 'mixtral-8x7b',
    name: 'Mixtral 8x7B',
    description: 'A high-quality sparse Mixture-of-Experts model (SMoE) by Mistral AI.',
    source: 'HuggingFace',
    size: 26,
    ram_required: 48,
    vram_required: 32,
    quantization: 'Q4_0',
    quality_score: 95,
    performance_score: 65,
    speed_score: 60,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: true,
    tags: ['mixture-of-experts', 'high-quality'],
  },
  {
    id: 'qwen-72b',
    name: 'Qwen 1.5 72B',
    description: 'A massive 72B parameter model from Alibaba Cloud, excelling in multilingual tasks.',
    source: 'Ollama',
    size: 41,
    ram_required: 64,
    vram_required: 48,
    quantization: 'Q4_K_M',
    quality_score: 94,
    performance_score: 55,
    speed_score: 45,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['multilingual', 'large', 'chat'],
  },
  {
    id: 'llava-13b',
    name: 'LLaVA 1.5 13B',
    description: 'A large multi-modal model that connects a vision encoder and a large language model for general-purpose visual and language understanding.',
    source: 'HuggingFace',
    size: 7.4,
    ram_required: 16,
    vram_required: 10,
    quantization: 'GGUF',
    quality_score: 88,
    performance_score: 70,
    speed_score: 75,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['vision', 'multimodal', 'chat'],
  },
  {
    id: 'llama3-70b',
    name: 'Llama 3 70B Instruct',
    description: 'The powerful 70B parameter language model from Meta, for highly complex tasks.',
    source: 'Ollama',
    size: 39,
    ram_required: 64,
    vram_required: 48,
    quantization: 'Q4_K_M',
    quality_score: 96,
    performance_score: 50,
    speed_score: 40,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: true,
    tags: ['general', 'chat', 'large', 'reasoning'],
  },
  {
    id: 'phi3-medium',
    name: 'Phi-3 Medium 14B',
    description: 'A high-performance 14B model from Microsoft, balancing capability and size.',
    source: 'Ollama',
    size: 7.9,
    ram_required: 16,
    vram_required: 12,
    quantization: 'Q4_K_M',
    quality_score: 88,
    performance_score: 72,
    speed_score: 80,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['medium', 'balanced', 'coding'],
  },
  {
    id: 'command-r-plus',
    name: 'Command R+ 104B',
    description: 'A scalable model by Cohere built for RAG and tool use in enterprise settings.',
    source: 'Ollama',
    size: 60,
    ram_required: 128,
    vram_required: 80,
    quantization: 'Q4_K_M',
    quality_score: 97,
    performance_score: 40,
    speed_score: 30,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['enterprise', 'rag', 'tool-use', 'multilingual'],
  },
  {
    id: 'gemma2-9b',
    name: 'Gemma 2 9B',
    description: 'The next generation of open models from Google, with class-leading performance.',
    source: 'HuggingFace',
    size: 5.5,
    ram_required: 16,
    vram_required: 8,
    quantization: 'GGUF',
    quality_score: 89,
    performance_score: 79,
    speed_score: 85,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['google', 'research', 'efficient', 'general'],
  },
  {
    id: 'codegemma-7b',
    name: 'CodeGemma 7B',
    description: 'A code-specialized version of the Gemma model from Google, for code completion and generation.',
    source: 'Ollama',
    size: 4.8,
    ram_required: 8,
    vram_required: 6,
    quantization: 'Q4_K_M',
    quality_score: 86,
    performance_score: 77,
    speed_score: 89,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['coding', 'google', 'specialized'],
  },
  {
    id: 'dbrx-132b',
    name: 'DBRX Instruct 132B',
    description: 'A large-scale, open, mixture-of-experts (MoE) model created by Databricks.',
    source: 'HuggingFace',
    size: 72,
    ram_required: 128,
    vram_required: 80,
    quantization: 'GGUF',
    quality_score: 98,
    performance_score: 35,
    speed_score: 25,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['mixture-of-experts', 'large', 'databricks', 'enterprise'],
  },
  {
    id: 'nous-hermes2-mixtral',
    name: 'Nous Hermes 2 Mixtral 8x7B',
    description: 'A fine-tuned version of Mixtral 8x7B, excelling at conversational and role-playing tasks.',
    source: 'Ollama',
    size: 26,
    ram_required: 48,
    vram_required: 32,
    quantization: 'Q4_K_M',
    quality_score: 95,
    performance_score: 66,
    speed_score: 62,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: true,
    tags: ['chat', 'role-play', 'fine-tune'],
  },
  {
    id: 'wizardcoder-python-34b',
    name: 'WizardCoder Python 34B',
    description: 'A powerful coding model specifically fine-tuned for Python programming tasks.',
    source: 'HuggingFace',
    size: 19.1,
    ram_required: 32,
    vram_required: 24,
    quantization: 'GGUF',
    quality_score: 93,
    performance_score: 62,
    speed_score: 55,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['coding', 'python', 'specialized'],
  },
  {
    id: 'tinylama-1.1b',
    name: 'TinyLlama 1.1B',
    description: 'A compact 1.1B Llama model, perfect for applications with extreme resource constraints.',
    source: 'Ollama',
    size: 0.6,
    ram_required: 4,
    vram_required: 2,
    quantization: 'Q4_K_M',
    quality_score: 65,
    performance_score: 95,
    speed_score: 98,
    cpu_performance_needed: 'low',
    gpu_performance_needed: 'low',
    apple_silicon_support: true,
    tags: ['small', 'fast', 'research', 'on-device'],
  },
    {
    id: 'gemma2-27b',
    name: 'Gemma 2 27B',
    description: 'The larger 27B parameter version of Google\'s Gemma 2 model series for complex tasks.',
    source: 'HuggingFace',
    size: 15,
    ram_required: 32,
    vram_required: 20,
    quantization: 'GGUF',
    quality_score: 94,
    performance_score: 68,
    speed_score: 65,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: true,
    tags: ['google', 'research', 'large'],
  },
  {
    id: 'orca2-13b',
    name: 'Orca 2 13B',
    description: 'A 13B model from Microsoft Research that uses a novel approach to improve reasoning abilities.',
    source: 'Ollama',
    size: 7.4,
    ram_required: 16,
    vram_required: 10,
    quantization: 'Q4_K_M',
    quality_score: 87,
    performance_score: 73,
    speed_score: 78,
    cpu_performance_needed: 'mid',
    gpu_performance_needed: 'mid',
    apple_silicon_support: true,
    tags: ['reasoning', 'research', 'microsoft'],
  },
  {
    id: 'falcon-180b',
    name: 'Falcon 180B',
    description: 'A massive 180B parameter model from TII, one of the largest open-source models available.',
    source: 'HuggingFace',
    size: 98,
    ram_required: 128,
    vram_required: 80,
    quantization: 'GGUF',
    quality_score: 97,
    performance_score: 20,
    speed_score: 15,
    cpu_performance_needed: 'high',
    gpu_performance_needed: 'high',
    apple_silicon_support: false,
    tags: ['large', 'research', 'multilingual'],
  }
];
